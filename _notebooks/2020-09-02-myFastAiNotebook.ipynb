{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"FastAI book\"\n",
    "> some ideas and thoughts to dive in later.\n",
    "\n",
    "- toc: true \n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [AI]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. chapter1:\n",
    "    - Enlitic company?\n",
    "    - The basic idea is to teach the whole game.\n",
    "    - [must read](https://arxiv.org/abs/1511.06434)\n",
    "    - Most of the time, however, picking an architecture isn't a very important part of the deep learning process. It's something that academics love to talk about, but in practice it is unlikely to be something you need to spend much time on.\n",
    "    - \n",
    "2. chapter2:\n",
    "    -\n",
    "3. chapter3:\n",
    "    - feedbackloops: explor/exploit in ml :-?\n",
    "    - baselines: nice idea\n",
    "    - funny ipy widgets\n",
    "    - My idea: nlp which convert meanings to better sentences that means it if i write some bulshit it converts it to a better sentence\n",
    "\n",
    "4. chapter4:\n",
    "    - negative log likelihood o softmax o crossentropy ...\n",
    "\n",
    "5. chapter5:\n",
    "    - didnt attend\n",
    "\n",
    "6. chapter6:\n",
    "    - lr_find\n",
    "    - fit_one_cycle vs fit\n",
    "\n",
    "7. chapter7:\n",
    "    - in embedin neurals we are using sgd on inputs not weights? wtf\n",
    "\n",
    "8. chapter8:\n",
    "    - paper: uda data augmentation\n",
    "    - paper: what you never wanted to know about floating point ....\n",
    "        * numbers near 0 are good\n",
    "    - class: activaitonstats for monitoring :-?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. more felexible way to load any data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* these two codes are the same(cheat sheet)\n",
    "```python\n",
    "tfms = [[Tokenizer.from_folder(path), Numericalize], [parent_label, Categorize]]\n",
    "files = get_text_files(path, folders = ['train', 'test'])\n",
    "splits = GrandparentSplitter(valid_name='test')(files)\n",
    "dsets = Datasets(files, tfms, splits=splits)\n",
    "dls = dsets.dataloaders(dl_type=SortedDL, before_batch=pad_input)\n",
    "```\n",
    "using datablocks\n",
    "```python\n",
    "path = untar_data(URLs.IMDB)\n",
    "dls = DataBlock(\n",
    "    blocks=(TextBlock.from_folder(path),CategoryBlock),\n",
    "    get_y = parent_label,\n",
    "    get_items=partial(get_text_files, folders=['train', 'test']),\n",
    "    splitter=GrandparentSplitter(valid_name='test')\n",
    ").dataloaders(path)\n",
    "```\n",
    "some hooks to applay transforms\n",
    "after_item:: Applied on each item after grabbing it inside the dataset. This is the equivalent of item_tfms in DataBlock.\n",
    "before_batch:: Applied on the list of items before they are collated. This is the ideal place to pad items to the same size.\n",
    "after_batch:: Applied on the batch as a whole after its construction. This is the equivalent of batch_tfms in DataBlock.\n",
    "\n",
    "* it needs more config and transformers than using DataBlock API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
